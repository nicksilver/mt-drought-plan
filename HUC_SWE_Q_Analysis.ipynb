{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://snowex-hackweek.github.io/website/tutorials/geospatial/SNOTEL_query.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ulmo\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import geopandas as gpd\n",
    "from shapely.geometry import Point\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "WSDURL = 'https://hydroportal.cuahsi.org/Snotel/cuahsi_1_1.asmx?WSDL'\n",
    "START = '2004-01-01'\n",
    "END = '2021-12-31'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper functions\n",
    "def snotel_swe_fetch(site_code):\n",
    "    values_df = None\n",
    "    try: \n",
    "        site_values = ulmo.cuahsi.wof.get_values(\n",
    "            WSDURL, \n",
    "            site_code, \n",
    "            'SNOTEL:WTEQ_D',\n",
    "            start=START, \n",
    "            end=END)\n",
    "        values_df = pd.DataFrame.from_dict(site_values['values'])\n",
    "        values_df = values_df.set_index('datetime')\n",
    "        values_df.index = pd.to_datetime(values_df.index) \n",
    "        \n",
    "        # Convert values to float and replace -9999 nodata values with NaN\n",
    "        values_df['value'] = pd.to_numeric(values_df['value']).replace(-9999, np.nan)\n",
    "        \n",
    "        # Remove any records flagged with lower quality\n",
    "        values_df = values_df[values_df['quality_control_level_code'] == '1']\n",
    "        \n",
    "        # Add year column\n",
    "        values_df['year'] =  values_df.index.year\n",
    "    except:\n",
    "        print(\"Unable to fetch SWE for %s\" % site_code)\n",
    "        \n",
    "    return values_df[['value', 'year']]\n",
    "\n",
    "def usgs_q_fetch(site_code):\n",
    "    values_df = pd.DataFrame()\n",
    "    huc = None\n",
    "    try:\n",
    "        site_values = ulmo.usgs.nwis.get_site_data( \n",
    "            site_code=site_code,\n",
    "            service='daily',\n",
    "            start=START, \n",
    "            end=END)\n",
    "        if site_values:\n",
    "            if '00060:00003' in list(site_values.keys()):\n",
    "                values_df = pd.DataFrame.from_dict(site_values['00060:00003']['values'])\n",
    "                huc = site_values['00060:00003']['site']['huc']\n",
    "                values_df = values_df.set_index('datetime')\n",
    "                values_df.index = pd.to_datetime(values_df.index) \n",
    "                del values_df['qualifiers']\n",
    "                values_df = values_df.astype(float)\n",
    "        \n",
    "                # Convert values to float and replace -999999 nodata values with NaN\n",
    "                values_df['value'] = pd.to_numeric(values_df['value']).replace(-999999, np.nan)\n",
    "\n",
    "                # Add year column\n",
    "                values_df['year'] =  values_df.index.year\n",
    "\n",
    "                # Add season column\n",
    "                values_df['variable'] = values_df.index.to_series().dt.month.map(lambda x: season(x))\n",
    "                values_df = values_df.loc[values_df['variable']!='Other']\n",
    "            else:\n",
    "                print(\"This gage does not report mean discharge\")\n",
    "        else:\n",
    "            print('There are no data for this gage')\n",
    "            return huc, values_df\n",
    "    except:\n",
    "        print(\"Unable to fetch Q for %s\" % site_code)\n",
    "\t# # Sum over the season\n",
    "\t# values_df = values_df.groupby(['year', 'season']).sum().unstack()\n",
    "    return huc, values_df\n",
    "\n",
    "def season(x):\n",
    "    if x in range(5, 7):\n",
    "       return 'Early_Q'\n",
    "    if x in range(7, 9):\n",
    "       return 'Late_Q'\n",
    "    else :\n",
    "       return 'Other'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bring in HUC08s\n",
    "huc8_path = \"/media/nick/Seagate Backup Plus Drive/Data/HUCs/mt_huc8_4326.shp\"\n",
    "huc8 = gpd.read_file(huc8_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/nick/miniconda3/envs/general/lib/python3.9/site-packages/pandas/core/dtypes/cast.py:118: ShapelyDeprecationWarning: The array interface is deprecated and will no longer work in Shapely 2.0. Convert the '.coords' to a numpy array instead.\n",
      "  arr = construct_1d_object_array_from_listlike(values)\n"
     ]
    }
   ],
   "source": [
    "# Bring in SNOTEL site information\n",
    "sites = ulmo.cuahsi.wof.get_sites(WSDURL)\n",
    "sites_df = pd.DataFrame.from_dict(sites, orient='index').dropna()\n",
    "\n",
    "# Filter Montana sites\n",
    "sites_mt = sites_df.loc[sites_df['site_property'].map(lambda row: row['state']) == 'Montana']\n",
    "\n",
    "# Convert to GeoDataframe\n",
    "sites_mt_gdf = sites_mt.copy()\n",
    "sites_mt_gdf['geometry'] = [Point(float(loc['longitude']), float(loc['latitude'])) for loc in sites_mt['location']]\n",
    "sites_mt_gdf = gpd.GeoDataFrame(sites_mt_gdf, crs='EPSG:4326')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find hucs with SNOTEL stations\n",
    "huc8_snotels = sites_mt_gdf.sjoin(huc8, how=\"inner\")\n",
    "huc8_snotels_clean = huc8_snotels[['name', 'HUC8']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write and read HUC/SNOTEL key\n",
    "huc8_path = \"./data/huc8_snotels_mt.csv\"\n",
    "huc8_snotels_clean.to_csv(huc8_path)\n",
    "huc8_snotels_clean = pd.read_csv(huc8_path, index_col=0, dtype={'HUC8': str})\n",
    "huc8_snotels_clean.index = huc8_snotels.index.set_names(['stat'ion'])\n",
    "huc8_snotels_clean.reset_index(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "making request for sites: http://waterservices.usgs.gov/nwis/dv/\n",
      "processing data from request: https://waterservices.usgs.gov/nwis/dv/?format=waterml&stateCd=MT\n",
      "making request for sites: http://waterservices.usgs.gov/nwis/iv/\n",
      "processing data from request: https://waterservices.usgs.gov/nwis/iv/?format=waterml&stateCd=MT\n"
     ]
    }
   ],
   "source": [
    "# Bring in stream gage information\n",
    "gages = ulmo.usgs.nwis.get_sites(state_code='MT')\n",
    "gages_df = pd.DataFrame.from_dict(gages, orient='index').dropna()\n",
    "\n",
    "# Filter out gages that are in hucs without snotel stations\n",
    "gages_df = gages_df[gages_df['huc'].isin(huc8_snotels_clean['HUC8'])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Process USGS stream gage data\n",
    "site_codes = gages_df['code'].values \n",
    "start_yr = int(START.split('-')[0])\n",
    "end_yr = int(END.split('-')[0])\n",
    "q_sum_full = pd.DataFrame()\n",
    "for site_code in site_codes:\n",
    "\ttry:\n",
    "\t\thuc, q = usgs_q_fetch(site_code)\n",
    "\t\tif not q.empty :\n",
    "\t\t\tq_sum = q.groupby(['year', 'variable']).sum().unstack()\n",
    "\t\t\tq_sum.rename({'value': huc}, axis=1, inplace=True)\n",
    "\t\t\tq_sum_full = pd.concat([q_sum_full, q_sum.T.reset_index()], ignore_index=True)\n",
    "\t\t\tprint(\"Done fetching Q for %s\" % site_code)\n",
    "\t\telse:\n",
    "\t\t\tcontinue\n",
    "\texcept Exception as e:\n",
    "\t\tprint(e)\n",
    "\t\tcontinue\n",
    "\n",
    "# Filter out gages with NaN\n",
    "q_sum_full = q_sum_full.dropna(axis=0)\n",
    "\n",
    "# Remove column name\n",
    "q_sum_full.columns.name = None\n",
    "\n",
    "# Rename HUC column\n",
    "q_sum_full.rename({'level_0': 'HUC8'}, axis=1, inplace=True)\n",
    "\n",
    "# Sum values for each HUC and season\n",
    "q_sum_full = q_sum_full.groupby(['HUC8', 'variable']).sum().reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write and read discharge data to csv\n",
    "q_path = \"data/usgs_mt_huc_early_late_q.csv\"\n",
    "q_sum_full.to_csv(q_path)\n",
    "q_sum_full = pd.read_csv(q_path, index_col=0, dtype={'HUC8': str})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find annual max SWE for all MT snotel stations\n",
    "start_yr = int(START.split('-')[0])\n",
    "end_yr = int(END.split('-')[0])\n",
    "swe_max_full = pd.DataFrame()\n",
    "site_codes = sites_mt_gdf.index\n",
    "for site_code in site_codes:\n",
    "\ttry:\n",
    "\t\tswe = snotel_swe_fetch(site_code)\n",
    "\t\tswe_max = swe.groupby('year').max()\n",
    "\t\tswe_max.rename({'value': site_code}, axis=1, inplace=True)\n",
    "\t\tswe_max_full = pd.concat([swe_max_full, swe_max.T.reset_index()], ignore_index=True)\n",
    "\t\t# swe_max_full = swe_max_full.merge(swe_max, left_index=True, right_index=True, how='left')\n",
    "\t\tprint(\"Done fetching SWE for %s\" % site_code)\n",
    "\texcept:\n",
    "\t\tcontinue\n",
    "\n",
    "# Filter out gages with NaN\n",
    "swe_max_full = swe_max_full.dropna(axis=0)\n",
    "\n",
    "# Remove column name\n",
    "swe_max_full.columns.name = None\n",
    "\n",
    "# Rename station_id column\n",
    "swe_max_full.rename({'index': 'station_id'}, axis=1, inplace=True)\n",
    "\n",
    "# Find HUC associated with each station \n",
    "swe_max_full = swe_max_full.merge(\n",
    "    huc8_snotels_clean, \n",
    "    left_on='station_id', \n",
    "    right_on='station', \n",
    "    how='left')\n",
    "\n",
    "# Drop unnecessary columns\n",
    "swe_max_full.drop(['station_id', 'station', 'name'], axis=1, inplace=True)\n",
    "\n",
    "# Calculate average SWE for each HUC8\n",
    "swe_max_full = swe_max_full.groupby('HUC8').mean().reset_index()\n",
    "\n",
    "# Add variable column\n",
    "swe_max_full['variable'] = np.repeat('SWE_Max', len(swe_max_full))\n",
    "\n",
    "# Re-order columns and make sure they are strings\n",
    "swe_cols = swe_max_full.columns.tolist()\n",
    "swe_cols = [swe_cols[0]] + swe_cols[-1:] + swe_cols[1:-1]\n",
    "swe_max_full = swe_max_full[swe_cols]\n",
    "swe_max_full.columns = [str(i) for i in swe_cols]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Concatenate all data into one DataFrame\n",
    "final_df = pd.concat([q_sum_full, swe_max_full], ignore_index=True)\n",
    "\n",
    "# Drop HUCS that do not have all variables\n",
    "for huc in final_df['HUC8'].unique().tolist():\n",
    "    nvar = len(final_df[final_df['HUC8'] == huc])\n",
    "    if nvar < 3:\n",
    "        final_df.drop(final_df.index[final_df['HUC8'] == huc], inplace=True)\n",
    "\n",
    "# Sort dataframe by HUC8 and variable        \n",
    "final_df.sort_values(['HUC8', 'variable'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save final dataset \n",
    "swe_path = './data/swe_q_huc_final.csv'\n",
    "final_df.to_csv(swe_path, index=False)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "66243b74f7584dcf523dd7d5230c25ee826cc595b8803f8097d098ba2dcad588"
  },
  "kernelspec": {
   "display_name": "Python 3.8.10 64-bit ('venv': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
